{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elo_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NirDhhJ-zwGz"
      },
      "source": [
        "**Import all the libraries :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwfLFk9DzkNn"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import pickle\n",
        "import zipfile\n",
        "import warnings \n",
        "import datetime\n",
        "import lightgbm\n",
        "import prettytable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "sns.set_style(\"whitegrid\")\n",
        "from functools import reduce\n",
        "import matplotlib.pylab as plt\n",
        "warnings.filterwarnings('ignore')\n",
        "from IPython.display import Image\n",
        "from sklearn import preprocessing\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gITWy9wn1Qve"
      },
      "source": [
        "**Loading all Functions :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhN0ksWJ1QFU"
      },
      "source": [
        "def loadData():\n",
        "\n",
        "\n",
        "  train_data       = pd.read_csv('/content/train.csv', parse_dates=[\"first_active_month\"])\n",
        "  test_data        = pd.read_csv('/content/test.csv', parse_dates=[\"first_active_month\"])\n",
        "  historical_data  = pd.read_csv('/content/historical_transactions.csv',parse_dates=['purchase_date'])\n",
        "  newmerchant_data = pd.read_csv('/content/new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])\n",
        "  \n",
        "  return train_data, test_data, newmerchant_data, historical_data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9xs5WPc2l9G"
      },
      "source": [
        "# Reference: https://www.kaggle.com/rinnqd/reduce-memory-usage\n",
        "\n",
        "def reduce_memory_usage(df, verbose=True):\n",
        "  '''\n",
        "  The data size is too big to get rid of memory error this method will reduce memory\n",
        "  usage by changing types. It does the following\n",
        "  - Load objects as categories\n",
        "  - Binary values are switched to int8\n",
        "  - Binary values with missing values are switched to float16\n",
        "  - 64 bits encoding are all switched to 32 or 16bits if possible.\n",
        "  \n",
        "  Parameters :\n",
        "  df - DataFrame whose size to be reduced\n",
        "  verbose - Boolean, to mention the verbose required or not.\n",
        "  '''\n",
        "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "  start_mem = df.memory_usage().sum() / 1024**2\n",
        "  for col in df.columns:\n",
        "      col_type = df[col].dtypes\n",
        "      if col_type in numerics:\n",
        "          c_min = df[col].min()\n",
        "          c_max = df[col].max()\n",
        "          if str(col_type)[:3] == 'int':\n",
        "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                  df[col] = df[col].astype(np.int8)\n",
        "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                  df[col] = df[col].astype(np.int16)\n",
        "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                  df[col] = df[col].astype(np.int32)\n",
        "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                  df[col] = df[col].astype(np.int64)\n",
        "          else:\n",
        "              c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
        "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
        "                  df[col] = df[col].astype(np.float16)\n",
        "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
        "                  df[col] = df[col].astype(np.float32)\n",
        "              else:\n",
        "                  df[col] = df[col].astype(np.float64)\n",
        "  end_mem = df.memory_usage().sum() / 1024**2\n",
        "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LWMuNUkxY8q"
      },
      "source": [
        "def baseline_features(train_data,test_data,historical_data,newmerchant_data):\n",
        "   '''This function is for adding basic features\n",
        "      on the data.\n",
        "      parameters:\n",
        "      - train_data: dataframe for train.csv\n",
        "      - test_data: dataframe for test.csv\n",
        "      - historical_data: dataframe for historical_transactions.csv\n",
        "      - newmerchant_data: dataframe for new_merchant_transaction.csv'''\n",
        "\n",
        "  #1. Transform first_active_month and extract year and month :\n",
        "\n",
        "  # In train_data\n",
        "  train_data['first_active_month'] = pd.to_datetime(train_data['first_active_month'])\n",
        "\n",
        "  # In test_data\n",
        "  test_data['first_active_month'] = pd.to_datetime(test_data['first_active_month'])\n",
        "\n",
        "  for df in [train_data, test_data]:\n",
        "    # extracting the year and month\n",
        "    df['first_active_year'] = df['first_active_month'].dt.year.values\n",
        "    df['first_active_mon'] = df['first_active_month'].dt.month.values\n",
        "\n",
        "  # Encode first_active_year column\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  train_data['first_active_year'] = le.fit_transform(train_data['first_active_year'] )\n",
        "  test_data['first_active_year'] = le.fit_transform(test_data['first_active_year'] )\n",
        "\n",
        "  # Encode first_active_mon column\n",
        "  train_data['first_active_mon'] = le.fit_transform(train_data['first_active_mon'] )\n",
        "  test_data['first_active_mon'] = le.fit_transform(test_data['first_active_mon'] )\n",
        "\n",
        "  #2. Derive last purchase amount, last active month and dormancy feature :\n",
        "\n",
        "  ## For historical transaction\n",
        "  historical_data['purchase_date'] = pd.to_datetime(historical_data['purchase_date'])\n",
        "\n",
        "  # last active month & last purchase amount\n",
        "  last_active_month = historical_data.loc[historical_data.groupby('card_id').purchase_date.idxmax(),:][['card_id','purchase_date','purchase_amount']]\n",
        "  last_active_month.columns = ['card_id','hist_transc_last_active_purchase_date','hist_transc_last_active_purchase_amount']\n",
        "  train_data = pd.merge(train_data,last_active_month, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data,last_active_month, on=\"card_id\",how='left')\n",
        "\n",
        "  train_data['hist_transc_last_active_purchase_month'] = train_data['hist_transc_last_active_purchase_date'].dt.month\n",
        "  test_data['hist_transc_last_active_purchase_month'] = test_data['hist_transc_last_active_purchase_date'].dt.month\n",
        "\n",
        "  # dormancy feature\n",
        "  max_purchase_date_hist = historical_data['purchase_date'].max()\n",
        "  train_data['hist_transc_dormancy'] = [(max_purchase_date_hist-x).days for x in train_data['hist_transc_last_active_purchase_date']]\n",
        "  test_data['hist_transc_dormancy'] = [(max_purchase_date_hist-x).days for x in test_data['hist_transc_last_active_purchase_date']]\n",
        "  train_data.head()\n",
        "\n",
        "  ## For new_merchant_transaction\n",
        "  newmerchant_data['purchase_date'] = pd.to_datetime(newmerchant_data['purchase_date'])\n",
        "\n",
        "  # last active month & last purchase amount\n",
        "  last_active_month = newmerchant_data.loc[newmerchant_data.groupby('card_id').purchase_date.idxmax(),:][['card_id','purchase_date','purchase_amount']]\n",
        "  last_active_month.columns = ['card_id','new_transc_last_active_purchase_date','new_transc_last_active_purchase_amount']\n",
        "  train_data = pd.merge(train_data, last_active_month, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data, last_active_month, on=\"card_id\",how='left')\n",
        "\n",
        "  train_data['new_transc_last_active_purchase_month'] = train_data['new_transc_last_active_purchase_date'].dt.month\n",
        "  test_data['new_transc_last_active_purchase_month'] = test_data['new_transc_last_active_purchase_date'].dt.month\n",
        "\n",
        "  # dormancy feature\n",
        "  max_purchase_date_newmer = newmerchant_data['purchase_date'].max()\n",
        "  train_data['new_transc_dormancy'] = [(max_purchase_date_newmer-x).days for x in train_data['new_transc_last_active_purchase_date']]\n",
        "  test_data['new_transc_dormancy'] = [(max_purchase_date_newmer-x).days for x in test_data['new_transc_last_active_purchase_date']]\n",
        "\n",
        "  #3. Deriving Favourite merchant and Number of transactions at Favourite merchant as feature :\n",
        "\n",
        "  # For historical transaction\n",
        "  merchantid_counts_percard = pd.DataFrame(historical_data[['card_id','merchant_id']].groupby(['card_id','merchant_id']).size())\n",
        "  merchantid_counts_percard.columns = ['num_favourite_merchant']\n",
        "  merchantid_counts_percard = merchantid_counts_percard.sort_values(by='num_favourite_merchant',ascending=False)\n",
        "  merchantid_counts_percard = merchantid_counts_percard.groupby(level=0).head(1).reset_index()\n",
        "  merchantid_counts_percard.columns = ['card_id','hist_transc_favourite_merchant','hist_transc_num_transaction_favourite_merchant']\n",
        "  train_data = pd.merge(train_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "\n",
        "  # Encode Favourite merchant column\n",
        "  train_data['hist_transc_favourite_merchant'] = le.fit_transform(train_data['hist_transc_favourite_merchant'] )\n",
        "  test_data['hist_transc_favourite_merchant'] = le.fit_transform(test_data['hist_transc_favourite_merchant'] )\n",
        "\n",
        "  # For new_merchant_transaction\n",
        "  merchantid_counts_percard = pd.DataFrame(newmerchant_data[['card_id','merchant_id']].groupby(['card_id','merchant_id']).size())\n",
        "  merchantid_counts_percard.columns = ['num_favourite_merchant']\n",
        "  merchantid_counts_percard = merchantid_counts_percard.sort_values(by='num_favourite_merchant',ascending=False)\n",
        "  merchantid_counts_percard = merchantid_counts_percard.groupby(level=0).head(1).reset_index()\n",
        "  merchantid_counts_percard.columns = ['card_id','new_transc_favourite_merchant','new_transc_num_transaction_favourite_merchant']\n",
        "  train_data = pd.merge(train_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "\n",
        "  train_data['new_transc_favourite_merchant'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "  test_data['new_transc_favourite_merchant'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "\n",
        "  # Encode Favourite merchant column\n",
        "  train_data['new_transc_favourite_merchant'] = le.fit_transform(train_data['new_transc_favourite_merchant'] )\n",
        "  test_data['new_transc_favourite_merchant'] = le.fit_transform(test_data['new_transc_favourite_merchant'] )\n",
        "\n",
        "  return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6-G5cltpymA"
      },
      "source": [
        "def imputation(df_hist, df_new):\n",
        "  '''This function is for missing value imputation in data\n",
        "     parameters:\n",
        "     - df_test: test_data\n",
        "     - df_hist: historical_data\n",
        "     - df_new: newmerch_data.'''\n",
        "\n",
        "  # In historical_data\n",
        "  df_hist['category_2'].fillna(1,inplace=True)# I put '1' here because it is most occured value in this feature\n",
        "  df_hist['category_3'].fillna('A',inplace=True)# I put 'A' here because of most count value\n",
        "  df_hist['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)# same merchant_id is also been taken on the basis of count.\n",
        "\n",
        "  # I use same techniques for newmerchant_data\n",
        "  df_new['category_3'].fillna('A',inplace=True)\n",
        "  df_new['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "  df_new['category_2'].fillna(1.0,inplace=True)\n",
        "\n",
        "  return df_hist, df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PizQJAgY4p91"
      },
      "source": [
        "def encode_categorical(df):\n",
        "  '''This function is specially for encode the categorical values of \n",
        "     transactions data\n",
        "     parameters:\n",
        "     - df: the Dataframe where the label encoding will performed on certain features'''\n",
        "  \n",
        "  ## label encode the categorical variables.\n",
        "  e = {'N':0, 'Y':1}\n",
        "  df['category_1']= df['category_1'].map(e)\n",
        "  df['authorized_flag']= df['authorized_flag'].map(e)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Wwywzn7mST"
      },
      "source": [
        "def oneHotEncoding(df, features, original_df):\n",
        "  '''This function is for one-hot encoding the categorical features\n",
        "     parameters:\n",
        "     - df: DataFrame\n",
        "     - features: Features needs to be one hot encoded.'''\n",
        "\n",
        "  for feat in features:\n",
        "    unique_values = original_df[feat].unique()\n",
        "\n",
        "    for cat in unique_values:\n",
        "      df[feat+'={}'.format(cat)] = (df[feat] == cat).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIPy03uA7mPQ"
      },
      "source": [
        "def aggregated_features(new_df, df, aggs, grpby, name='',  prefix='', use_col=False):\n",
        "  '''\n",
        "  This function is to find the \n",
        "  aggregated values (sum,min,max,std,median,mean,nunique) for a columns aggregated by the groupby operation\n",
        "  \n",
        "  Parameters:\n",
        "  new_df   - features will be added to this DF\n",
        "  df       - original DF from which the features will be created\n",
        "  grpby    - based on this column we'll to group by\n",
        "  name     - name for the new features created\n",
        "  aggs     - dictionary contains key as the column the operation performed and list of operations as the value.\n",
        "  prefix   - added to the name of the feature -- default value empty\n",
        "  use_col  - if set True then the original column name will be uesd to name the new feature -- default value False\n",
        "  '''\n",
        "  # boolean for using the original column name in the aggregated features\n",
        "  # iterating through the columns of the need to be aggregated \n",
        "\n",
        "  for col, funcs in aggs.items():\n",
        "    for func in funcs:\n",
        "        # Getting the name of aggregation function\n",
        "        if isinstance(func, str):\n",
        "            func_str = func\n",
        "        else:\n",
        "            func_str = func.__name__ \n",
        "        # create the column\n",
        "        if use_col:\n",
        "          name = prefix+'_'+col+'_'+'{}'.format(func_str)\n",
        "\n",
        "        new_df[name] = df.groupby([grpby])[col].agg(func).values\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYmB7f798nlu"
      },
      "source": [
        "def data_preprocessing(historical_data,newmerchant_data):\n",
        "  '''This function is for performing preprocessing \n",
        "  practices on the data.\n",
        "  parameters:\n",
        "  historical_data: data from historical_transaction\n",
        "  newmerchant_data: data from nemerch_transaction'''\n",
        "    \n",
        "  ## imputing the missing values\n",
        "  print(' - Imputing Missing values...')\n",
        "  historical_data, newmerchant_data = imputation(historical_data,newmerchant_data)\n",
        "  \n",
        "  ## encoding the categorical features in historical transactions\n",
        "  historical_data = encode_categorical(historical_data)\n",
        "\n",
        "  ## encoding the categorical features in new_merchants\n",
        "  newmerchant_data = encode_categorical(newmerchant_data)\n",
        "\n",
        "  ## One-hot encoding the categorical features\n",
        "  categorical_features = ['category_2','category_3','month_lag']\n",
        "\n",
        "  ## one-hot encoding historical transactions\n",
        "  print(' - One Hot Encoding of variables...')\n",
        "  oneHotEncoding(historical_data, features=categorical_features, original_df = historical_df)\n",
        "\n",
        "  ## one-hot encoding new merchants transactions\n",
        "  oneHotEncoding(newmerchant_data, features=categorical_features, original_df = newmerchant_df)\n",
        "  \n",
        "\n",
        "  ## calcuating month difference\n",
        "  reference_date = '2018-12-31'\n",
        "  reference_date = pd.to_datetime(reference_date)\n",
        "\n",
        "  # In historical_transactions \n",
        "  historical_data['month_diff'] = (reference_date - historical_data['purchase_date']).dt.days // (30 + historical_data['month_lag'])\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['month_diff'] = (reference_date - newmerchant_data['purchase_date']).dt.days // (30 + newmerchant_data['month_lag'])\n",
        "\n",
        "  ## Influential days feature\n",
        "  holidays = {'FathersDay_2017': '2017-08-13',\n",
        "              'ChildrenDay_2017':'2017-10-12',\n",
        "              'BlackFriday_2017':'2017-11-24',\n",
        "              'ValentineDay_2017':'2017-06-12',\n",
        "              'Republicday_2017':'2017-11-15',\n",
        "              'Independenceday_2017':'2017-09-7',\n",
        "              'EasterDay_2017' : '2017-04-16',\n",
        "              'AllSoulsDay_2017': '2017-11-2',\n",
        "              'ChristmasDay_2017': '2017-12-25'}\n",
        "\n",
        "  # In historical_transactions \n",
        "  for day, date in holidays.items():\n",
        "    historical_data[day] = (pd.to_datetime(date) - historical_data['purchase_date']).dt.days\n",
        "    historical_data[day] = historical_data[day].apply(lambda x: x if x > 0 and x < 15 else 0)\n",
        "  # In new_transactions\n",
        "  for day, date in holidays.items(): \n",
        "    newmerchant_data[day] = (pd.to_datetime(date) - newmerchant_data['purchase_date']).dt.days\n",
        "    newmerchant_data[day] = newmerchant_data[day].apply(lambda x: x if x > 0 and x < 15 else 0)\n",
        "\n",
        "  ## preprocess the purchase_amount\n",
        "  newmerchant_data['purchase_amount'] = np.round(newmerchant_data['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "  historical_data['purchase_amount'] = np.round(historical_data['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "\n",
        "  ## The ratio of purchase amount by month_diff is another feature which help to reveal the card_id's financial capcity and purchase_pattern.\n",
        "  # In historical_transactions \n",
        "  historical_data['amount_month_ratio'] = historical_data['purchase_amount'].values / (1.0 + historical_data['month_diff'].values)\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['amount_month_ratio'] = newmerchant_data['purchase_amount'].values / (1.0 + newmerchant_data['month_diff'].values)\n",
        "\n",
        "  ##is_weekend is a feature which purchase_date is weekend or weekday.\n",
        "  ##greater than 5 to check whether the day is sat or sunday then, if it is then assign a val 1 else 0\n",
        "  # In historical_transactions \n",
        "  historical_data['is_weekend'] = historical_data['purchase_date'].dt.dayofweek\n",
        "  historical_data['is_weekend'] = historical_data['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['is_weekend'] = newmerchant_data['purchase_date'].dt.dayofweek\n",
        "  newmerchant_data['is_weekend'] = newmerchant_data['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\n",
        "\n",
        "  ## extracting the day, hour , week from the purchase_date.\n",
        "  # In historical_transactions\n",
        "  historical_data['purchase_date_week'] = historical_data['purchase_date'].dt.week.values\n",
        "  historical_data['purchase_date_dayofweek'] = historical_data['purchase_date'].dt.dayofweek.values\n",
        "  historical_data['purchase_date_hour'] = historical_data['purchase_date'].dt.hour.values\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['purchase_date_week'] = newmerchant_data['purchase_date'].dt.week.values\n",
        "  newmerchant_data['purchase_date_dayofweek'] = newmerchant_data['purchase_date'].dt.dayofweek.values\n",
        "  newmerchant_data['purchase_date_hour'] = newmerchant_data['purchase_date'].dt.hour.values\n",
        "\n",
        "  return historical_data, newmerchant_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tssTzWCg-UqV"
      },
      "source": [
        "def feature_engineering(historical_data,newmerchant_data):\n",
        "  '''This function is for performing feature engineering\n",
        "     on the data.\n",
        "     parameters:\n",
        "     - historical_data: data from historical_transaction\n",
        "     - newmerchant_data: data from nemerch_transaction'''\n",
        "  \n",
        "  # In historical_transactions \n",
        "  features_historical = pd.DataFrame(historical_data.groupby(['card_id']).size()).reset_index()\n",
        "  features_historical.columns = ['card_id', 'hist_transc_count']\n",
        "  # # In new_merch_transactions \n",
        "  features_newmerch = pd.DataFrame(newmerchant_data.groupby(['card_id']).size()).reset_index()\n",
        "  features_newmerch.columns = ['card_id', 'new_transc_count']\n",
        "  \n",
        "  ## Aggregation all the id's\n",
        "  aggs = {'city_id':['nunique'],\n",
        "          'state_id' :['nunique'],\n",
        "          'merchant_category_id':['nunique'],\n",
        "          'subsector_id':['nunique'],\n",
        "          'merchant_id':['nunique']}\n",
        "\n",
        "  # In historical_transactions        \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions \n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation Installment feature \n",
        "  aggs = {'installments':['mean', 'sum', 'max', 'min', 'std', 'skew']}\n",
        "\n",
        "  # In historical_transactions        \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation purchase_amount feature\n",
        "  aggs = {'purchase_amount':['sum', 'mean', 'max', 'min', 'median', 'std', 'skew']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id', prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id', prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation month_lag feature \n",
        "  aggs = {'month_lag': ['nunique', 'mean', 'std', 'min', 'max', 'skew']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation purchase_date feature\n",
        "  aggs = {'purchase_date': ['max','min']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation category_1 & authorized_flag features\n",
        "  aggs = {'category_1':['sum', 'mean'],\n",
        "        'authorized_flag': ['sum', 'mean']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch,newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation category_2 & category_3 features\n",
        "  aggs = {'category_2=1.0':['sum', 'mean'],\n",
        "          'category_2=2.0':['sum', 'mean'],\n",
        "          'category_2=3.0':['sum', 'mean'],\n",
        "          'category_2=4.0':['sum', 'mean'],\n",
        "          'category_2=5.0':['sum', 'mean'],\n",
        "          'category_3=1.0':['sum', 'mean'],\n",
        "          'category_3=2.0':['sum', 'mean'],\n",
        "          'category_3=3.0':['sum', 'mean']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "  \n",
        "  ### Derived Features : from existing features\n",
        "\n",
        "  ## Authorized_flag\n",
        "  # historical_transactions\n",
        "  features_historical['hist_transc_denied_count'] = features_historical['hist_transc_count'].values - \\\n",
        "                                                    features_historical['hist_transc_authorized_flag_sum'].values\n",
        "  # new_merchant_transactions\n",
        "  features_newmerch['new_transc_denied_count'] = features_newmerch['new_transc_count'].values - \\\n",
        "                                                    features_newmerch['new_transc_authorized_flag_sum'].values\n",
        "\n",
        "  ## category_1\n",
        "  # historical_transactions\n",
        "  features_historical['hist_transc_category_1_sum_0'] = features_historical['hist_transc_count'].values - \\\n",
        "                                                              features_historical['hist_transc_category_1_sum'].values\n",
        "  # new_merchant_transactions\n",
        "  features_newmerch['new_transc_category_1_sum_0'] = features_newmerch['new_transc_count'].values - \\\n",
        "                                                            features_newmerch['new_transc_category_1_sum'].values\n",
        "  \n",
        "  ## find mean of the count of the transac for merchant id\n",
        "  # historical_transactions\n",
        "  features_historical['hist_transc_merchant_id_count_mean'] = features_historical['hist_transc_count'].values \\\n",
        "                                                                    / (1.0+features_historical['hist_transc_merchant_id_nunique'].values)\n",
        "  # new_merchant_transactions\n",
        "  features_newmerch['new_transc_merchant_id_count_mean'] = features_newmerch['new_transc_count'].values \\\n",
        "                                                              / (1.0+features_newmerch['new_transc_merchant_id_nunique'].values)\n",
        "  \n",
        "  ## In historical_transactions\n",
        "  # diff in purchase_date from max to min \n",
        "  features_historical['hist_transc_purchase_date_diff'] = (features_historical['hist_transc_purchase_date_max'] - features_historical['hist_transc_purchase_date_min']).dt.days.values\n",
        "  # purchase_count_ratio\n",
        "  features_historical['hist_transc_purchase_count_ratio'] = features_historical['hist_transc_count'].values / (1.0 + features_historical['hist_transc_purchase_date_diff'].values)\n",
        "\n",
        "  ## In new_merch_transactions\n",
        "  # diff in purchase_date from max to min \n",
        "  features_newmerch['new_transc_purchase_date_diff'] = (features_newmerch['new_transc_purchase_date_max'] - features_newmerch['new_transc_purchase_date_min']).dt.days.values\n",
        "  # purchase_count_ratio\n",
        "  features_newmerch['new_transc_purchase_count_ratio'] = features_newmerch['new_transc_count'].values / (1.0 + features_newmerch['new_transc_purchase_date_diff'].values)\n",
        "\n",
        "  ## aggregate features for is_weekend \n",
        "  aggs = {'is_weekend': ['sum','mean']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\n",
        "\n",
        "  ## aggregated features on  day, hour , week\n",
        "  aggs = {'purchase_date_week': ['nunique', 'mean', 'min', 'max'],\n",
        "          'purchase_date_dayofweek': ['nunique', 'mean', 'min', 'max'],\n",
        "          'purchase_date_hour':['nunique', 'mean', 'min', 'max']}\n",
        "\n",
        "  # In historical_transactions\n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## calculating the aggerated features like ['mean', 'min', 'max'] for month_diff column.\n",
        "  aggs = {'month_diff': ['mean', 'min', 'max']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id', prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id', prefix='new_transc', use_col=True)\n",
        "\n",
        "  # aggregated features on the amount ratio and month_lag.\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\n",
        "          'month_lag=0': ['sum','mean'],\n",
        "          'month_lag=-1':['sum','mean'],\n",
        "          'month_lag=-2':['sum','mean']}\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "\n",
        "  # aggregated features on the amount ratio and month_lag.\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\n",
        "          'month_lag=1': ['sum','mean'],\n",
        "          'month_lag=2':['sum','mean']}\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  # month_lag ratio in historical transactions\n",
        "  features_historical['hist_transc_month_lag_0_-1_ratio'] = features_historical['hist_transc_month_lag=0_sum'] \\\n",
        "                                                                  / (1.0 + features_historical['hist_transc_month_lag=-1_sum'])\n",
        "\n",
        "  features_historical['hist_transc_month_lag_0_-2_ratio'] = features_historical['hist_transc_month_lag=0_sum'] \\\n",
        "                                                                  / (1.0 + features_historical['hist_transc_month_lag=-2_sum'])\n",
        "\n",
        "  # ratio of the summed month lags with the transaction count\n",
        "  tmp = features_historical[['hist_transc_month_lag=0_sum','hist_transc_month_lag=-1_sum','hist_transc_month_lag=-2_sum']].sum(axis=1)\n",
        "\n",
        "  features_historical['hist_transc_month_lag_sum_ratio'] = tmp / (1.0 + features_historical['hist_transc_count'])\n",
        "\n",
        "  # month_lag ratio between two month_lags.\n",
        "  features_newmerch['new_transc_month_lag_1_2_ratio'] = features_newmerch['new_transc_month_lag=1_sum'] \\\n",
        "                                                                  / (1.0 + features_newmerch['new_transc_month_lag=2_sum'])\n",
        "\n",
        "  ## difference in the amount spend with cards\n",
        "  # In historical_transactions \n",
        "  features_historical['hist_transc_amount_diff'] = features_historical['hist_transc_purchase_amount_max'].values - features_historical['hist_transc_purchase_amount_min'].values\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch['new_transc_amount_diff'] = features_newmerch['new_transc_purchase_amount_max'].values - features_newmerch['new_transc_purchase_amount_min'].values\n",
        "\n",
        "  ## Influential days feature\n",
        "  holidays = {'FathersDay_2017': '2017-08-13',\n",
        "              'ChildrenDay_2017':'2017-10-12',\n",
        "              'BlackFriday_2017':'2017-11-24',\n",
        "              'ValentineDay_2017':'2017-06-12',\n",
        "              'Republicday_2017':'2017-11-15',\n",
        "              'Independenceday_2017':'2017-09-7',\n",
        "              'EasterDay_2017' : '2017-04-16',\n",
        "              'AllSoulsDay_2017': '2017-11-2',\n",
        "              'ChristmasDay_2017': '2017-12-25'}\n",
        "\n",
        "  ## aggregation of holidays\n",
        "  aggs = dict(zip(holidays.keys(),[['mean'] for x in holidays.keys()]))\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "  \n",
        "  return features_historical, features_newmerch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjx3vmn9-8W-"
      },
      "source": [
        "def merge_drop(df,features_historical,features_newmerch):\n",
        "   '''This function is for merging the engineered features and\n",
        "      removing the unnecessary features.\n",
        "      parameters:\n",
        "      - df: data for prediction\n",
        "      - features_historical: features from historical_transc\n",
        "      - features_newmerch: features from nemerch_transc'''\n",
        "\n",
        "  train = reduce(lambda left,right: pd.merge(left,right,on='card_id', how='left'), [df, features_historical, features_newmerch])\n",
        "  \n",
        "  remove_cols = ['first_active_month','new_transc_purchase_date_max',\n",
        " 'new_transc_purchase_date_min','hist_transc_purchase_date_max',\n",
        " 'hist_transc_purchase_date_min', 'hist_transc_last_active_purchase_date', 'new_transc_last_active_purchase_date']\n",
        "\n",
        "  train = train.drop(labels=remove_cols, axis = 1)\n",
        "\n",
        "  return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x20p9kJR-GH3"
      },
      "source": [
        "def root_mean_squared_error(X , y_true):\n",
        "  '''This function is for calculating the RMSE score\n",
        "  parameters:\n",
        "  X: test_data\n",
        "  y_true: original target score'''\n",
        "\n",
        "  y_predicted = predict_loyalty_score(X)\n",
        "  ## calculating the RMSE score\n",
        "  rmse = np.sqrt(mean_squared_error(y_predicted, y_true))\n",
        "  print('RMSE Score:', rmse)\n",
        "\n",
        "  return rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Im6LJD-iuo"
      },
      "source": [
        "def predict_loyalty_score(X):\n",
        "  ''' This function predict's loyalty Score of the given card_id/ids\n",
        "  parameters:\n",
        "  X : card_id/List of the card_ids\n",
        "  returns Dataframe with predicted loyalty score for card_id/ids.'''\n",
        "  \n",
        "\n",
        "  ## training the data\n",
        "  # Fetching the transactional and card_id data\n",
        "  print(\"Fetching the transactional and card_id data\")\n",
        "  sample = cards.loc[cards['card_id'].isin(X)]\n",
        "  newmerchant_data = newmerchant_df[newmerchant_df['card_id'].isin(X)]\n",
        "  historical_data = historical_df[historical_df['card_id'].isin(X)]\n",
        "\n",
        "  ## Performing preprocessing steps on transactions data\n",
        "  print(\"PreProcess the transactions data......\")\n",
        "  historical_data, newmerchant_data = data_preprocessing(historical_data,newmerchant_data)\n",
        "\n",
        "  ## Performing feature Engineering on transactions data\n",
        "  print(\"Feature Engineering the transactions data.....\")\n",
        "  features_historical, features_newmerch = feature_engineering(historical_data,newmerchant_data)\n",
        "\n",
        "  ## Performing preprocessing on Feature Engineered Data\n",
        "  print('preprocess the Feature Engineered Data')\n",
        "  query_point = merge_drop(sample,features_historical,features_newmerch)\n",
        "  query_point.set_index('card_id', inplace=True)\n",
        "\n",
        "  ## Predicting the Loyalty Score\n",
        "  print(\"Predicting the Loyalty Score.....\")\n",
        "  with open('/content/drive/MyDrive/Case study1/xgb_model_final.sav', 'rb') as pickle_file:\n",
        "      xgboost_model = pickle.load(pickle_file)\n",
        "  with open('/content/drive/MyDrive/Case study1/lgbm_model1.sav', 'rb') as pickle_file:\n",
        "    lgbm_model = pickle.load(pickle_file)\n",
        "  with open('/content/drive/MyDrive/Case study1/stacked_model1.sav', 'rb') as pickle_file:\n",
        "      stacked_model = pickle.load(pickle_file)\n",
        "\n",
        "  xgboost_predictions = xgboost_model.predict(xgb.DMatrix(query_point[xgboost_model.feature_names]), ntree_limit=xgboost_model.best_ntree_limit+50)  \n",
        "  lgbm_predictions = lgbm_model.predict(query_point, num_iteration=lgbm_model.best_iteration)\n",
        "  stacked_prediction = np.vstack([xgboost_predictions, lgbm_predictions]).transpose()\n",
        "  final_prediction = stacked_model.predict(stacked_prediction)\n",
        "\n",
        "  ## Preparing the output\n",
        "  Score_df = pd.DataFrame()\n",
        "  Score_df['card_id'] = query_point.index \n",
        "  Score_df['loyality_score'] = final_prediction\n",
        "  Score_df.set_index('card_id', inplace=True)\n",
        "  \n",
        "  return Score_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRUdIx-8ul2Q"
      },
      "source": [
        "**Downloading data :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iJhCFa31lKu",
        "outputId": "516d2425-def2-47fa-b052-0b315746d4cd"
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10445/200747/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1620215211&Signature=Mt2Std3ijid8ntLwh3anT4DJ2t%2Ba4JzlrlBq29yXAqnwJlCLBWq3qenMV6Pr6QE3ZaWIiZfBFknZQ7EcMNJJtYS2l5GrnCRu68uT%2FPaZSSWoOA4PzOBFMAwnrffU8q%2FRogni4R0j%2B%2FFBGz%2FnrHw%2B2jLG%2F4%2F7gEY%2F7jhrNptqbVJFnLPxAwGWmlU9eGutByplN3A7gfSlbGk5TybkOPPzCG%2F%2B5TSvDmkPzkyNZ3wa84dbsjCLYiSvWFT0I%2FsnazgiVXUKWJUjF6AnQ3NNBYqWrWO2Kjc0s%2F63ZIhr06MvDdbe3C%2BJgCKiL8KV7vffGjyMhq4m1bGSdaD01VyZg0x19w%3D%3D&response-content-disposition=attachment%3B+filename%3Delo-merchant-category-recommendation.zip\" -c -O 'elo-merchant-category-recommendation.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-03 09:52:38--  https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10445/200747/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1620215211&Signature=Mt2Std3ijid8ntLwh3anT4DJ2t%2Ba4JzlrlBq29yXAqnwJlCLBWq3qenMV6Pr6QE3ZaWIiZfBFknZQ7EcMNJJtYS2l5GrnCRu68uT%2FPaZSSWoOA4PzOBFMAwnrffU8q%2FRogni4R0j%2B%2FFBGz%2FnrHw%2B2jLG%2F4%2F7gEY%2F7jhrNptqbVJFnLPxAwGWmlU9eGutByplN3A7gfSlbGk5TybkOPPzCG%2F%2B5TSvDmkPzkyNZ3wa84dbsjCLYiSvWFT0I%2FsnazgiVXUKWJUjF6AnQ3NNBYqWrWO2Kjc0s%2F63ZIhr06MvDdbe3C%2BJgCKiL8KV7vffGjyMhq4m1bGSdaD01VyZg0x19w%3D%3D&response-content-disposition=attachment%3B+filename%3Delo-merchant-category-recommendation.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 64.233.183.128, 173.194.194.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 644665605 (615M) [application/zip]\n",
            "Saving to: ‘elo-merchant-category-recommendation.zip’\n",
            "\n",
            "elo-merchant-catego 100%[===================>] 614.80M   131MB/s    in 5.0s    \n",
            "\n",
            "2021-05-03 09:52:43 (123 MB/s) - ‘elo-merchant-category-recommendation.zip’ saved [644665605/644665605]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li1Ze41K1l4t",
        "outputId": "52fbde32-434a-4abf-ddaf-27e66ab1e484"
      },
      "source": [
        "!unzip elo-merchant-category-recommendation.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  elo-merchant-category-recommendation.zip\n",
            "  inflating: Data Dictionary.xlsx    \n",
            "  inflating: Data_Dictionary.xlsx    \n",
            "  inflating: historical_transactions.csv  \n",
            "  inflating: merchants.csv           \n",
            "  inflating: new_merchant_transactions.csv  \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP_rfA9Ojsmf"
      },
      "source": [
        "**loading data :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qjSneK3ukov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bbf6ac-505f-4a53-ed9c-68917774b022"
      },
      "source": [
        "%%time\n",
        "train_df, test_df, newmerchant_df, historical_df = loadData()\n",
        "\n",
        "train_df = reduce_memory_usage(train_df)\n",
        "test_df = reduce_memory_usage(test_df)\n",
        "historical_df = reduce_memory_usage(historical_df)\n",
        "newmerchant_df = reduce_memory_usage(newmerchant_df)\n",
        "\n",
        "historical_df['category_3'].replace({'A':1, 'B':2,'C':3}, inplace=True)\n",
        "newmerchant_df['category_3'].replace({'A':1, 'B':2,'C':3}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to  5.20 Mb (43.7% reduction)\n",
            "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
            "Mem. usage decreased to 2026.75 Mb (34.8% reduction)\n",
            "Mem. usage decreased to 136.66 Mb (34.8% reduction)\n",
            "CPU times: user 4min 3s, sys: 11.7 s, total: 4min 15s\n",
            "Wall time: 4min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kzOz6TxEoH2",
        "outputId": "fc74d2b1-05f2-451d-add2-8c70604587b6"
      },
      "source": [
        "%%time\n",
        "train_data, test_data = baseline_features(train_df,test_df,historical_df,newmerchant_df)\n",
        "\n",
        "target  = train_data[['card_id','target']]\n",
        "target.set_index('card_id', inplace =True)\n",
        "\n",
        "cards = pd.concat([train_data.drop(['target'] , axis= 1) , test_data] , axis = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 18s, sys: 4.52 s, total: 2min 23s\n",
            "Wall time: 2min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx2aZMBZqZvF"
      },
      "source": [
        "****Predicting Loyalty Score for a single card id of Test Data :****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbM60dVCqqfk"
      },
      "source": [
        "data_point = test_data.sample(1)['card_id'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSjmDimBq0dK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "1df59b4f-71fe-420a-f884-4b1c7a02cf4f"
      },
      "source": [
        "%%time\n",
        "predict_loyalty_score(data_point)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "CPU times: user 1.67 s, sys: 43.7 ms, total: 1.72 s\n",
            "Wall time: 1.67 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loyality_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>card_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C_ID_3c49e885d1</th>\n",
              "      <td>-0.519028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 loyality_score\n",
              "card_id                        \n",
              "C_ID_3c49e885d1       -0.519028"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ECshKMq2BE"
      },
      "source": [
        "**Predicting Loyalty Score for a list of Test Card_id's :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_mngQelrGMY"
      },
      "source": [
        "data_set = test_data.sample(1000)['card_id'].to_list()\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pA7zqYWrMR0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "09cb0008-d0a8-4466-bd43-b9b321627df9"
      },
      "source": [
        "%%time\n",
        "predict_loyalty_score(data_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "CPU times: user 5.4 s, sys: 144 ms, total: 5.55 s\n",
            "Wall time: 5.31 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loyality_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>card_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C_ID_6a2c823e5f</th>\n",
              "      <td>-2.008991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_a749f3a780</th>\n",
              "      <td>-1.708625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_cc9840274c</th>\n",
              "      <td>-0.770120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_163ab1d052</th>\n",
              "      <td>-0.730276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_12bc7a41f0</th>\n",
              "      <td>-0.506894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_a49ad5697c</th>\n",
              "      <td>-1.189587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_88c7388492</th>\n",
              "      <td>-1.060245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_999cc0d894</th>\n",
              "      <td>-0.711917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_d6c86b18cd</th>\n",
              "      <td>-1.596773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C_ID_32ad340ed9</th>\n",
              "      <td>-1.598337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 loyality_score\n",
              "card_id                        \n",
              "C_ID_6a2c823e5f       -2.008991\n",
              "C_ID_a749f3a780       -1.708625\n",
              "C_ID_cc9840274c       -0.770120\n",
              "C_ID_163ab1d052       -0.730276\n",
              "C_ID_12bc7a41f0       -0.506894\n",
              "...                         ...\n",
              "C_ID_a49ad5697c       -1.189587\n",
              "C_ID_88c7388492       -1.060245\n",
              "C_ID_999cc0d894       -0.711917\n",
              "C_ID_d6c86b18cd       -1.596773\n",
              "C_ID_32ad340ed9       -1.598337\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7vA9VrscoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d3b4e8-f7c4-4fbb-a7c2-07788d999baa"
      },
      "source": [
        "root_mean_squared_error(data_set, target.loc[target.index.isin(data_set)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "RMSE Score: 3.2518931751119102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpvFBtFrPfdE"
      },
      "source": [
        "**Note :** The Prediction time is 1.67 seconds. This latency can be reduced further by fetching the features of the transactions and card_id's from database."
      ]
    }
  ]
}