{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elo_deploy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g2occX7fy5y"
      },
      "source": [
        "**Import all the libraries :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-oeusrBdT_r"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import pickle\n",
        "import zipfile\n",
        "import warnings \n",
        "import datetime\n",
        "import lightgbm\n",
        "import prettytable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "sns.set_style(\"whitegrid\")\n",
        "from functools import reduce\n",
        "import matplotlib.pylab as plt\n",
        "warnings.filterwarnings('ignore')\n",
        "from IPython.display import Image\n",
        "from sklearn import preprocessing\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIuV-l6Tc--O",
        "outputId": "6521b390-fa6e-4cf8-edec-cd0d40612a4e"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUOehHC_omm2"
      },
      "source": [
        "import flask\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpj5dYGtf_BW"
      },
      "source": [
        "**Creating directories for flask :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrvNXtg1vcJW"
      },
      "source": [
        "os.mkdir('templates')\n",
        "os.mkdir('static')\n",
        "os.mkdir('static/css')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL8ITt76eIP_"
      },
      "source": [
        "# Reference: https://www.kaggle.com/rinnqd/reduce-memory-usage\n",
        "\n",
        "def reduce_memory_usage(df, verbose=True):\n",
        "  '''\n",
        "  The data size is too big to get rid of memory error this method will reduce memory\n",
        "  usage by changing types. It does the following\n",
        "  - Load objects as categories\n",
        "  - Binary values are switched to int8\n",
        "  - Binary values with missing values are switched to float16\n",
        "  - 64 bits encoding are all switched to 32 or 16bits if possible.\n",
        "  \n",
        "  Parameters :\n",
        "  df - DataFrame whose size to be reduced\n",
        "  verbose - Boolean, to mention the verbose required or not.\n",
        "  '''\n",
        "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "  start_mem = df.memory_usage().sum() / 1024**2\n",
        "  for col in df.columns:\n",
        "      col_type = df[col].dtypes\n",
        "      if col_type in numerics:\n",
        "          c_min = df[col].min()\n",
        "          c_max = df[col].max()\n",
        "          if str(col_type)[:3] == 'int':\n",
        "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                  df[col] = df[col].astype(np.int8)\n",
        "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                  df[col] = df[col].astype(np.int16)\n",
        "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                  df[col] = df[col].astype(np.int32)\n",
        "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                  df[col] = df[col].astype(np.int64)\n",
        "          else:\n",
        "              c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
        "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
        "                  df[col] = df[col].astype(np.float16)\n",
        "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
        "                  df[col] = df[col].astype(np.float32)\n",
        "              else:\n",
        "                  df[col] = df[col].astype(np.float64)\n",
        "  end_mem = df.memory_usage().sum() / 1024**2\n",
        "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8L2qDvIe1n4"
      },
      "source": [
        "def baseline_features(train_data,test_data,historical_data,newmerchant_data):\n",
        "   '''This function is for adding basic features\n",
        "      on the data.\n",
        "      parameters:\n",
        "      - train_data: dataframe for train.csv\n",
        "      - test_data: dataframe for test.csv\n",
        "      - historical_data: dataframe for historical_transactions.csv\n",
        "      - newmerchant_data: dataframe for new_merchant_transaction.csv'''\n",
        "\n",
        "  #1. Transform first_active_month and extract year and month :\n",
        "\n",
        "  # In train_data\n",
        "  train_data['first_active_month'] = pd.to_datetime(train_data['first_active_month'])\n",
        "\n",
        "  # In test_data\n",
        "  test_data['first_active_month'] = pd.to_datetime(test_data['first_active_month'])\n",
        "\n",
        "  for df in [train_data, test_data]:\n",
        "    # extracting the year and month\n",
        "    df['first_active_year'] = df['first_active_month'].dt.year.values\n",
        "    df['first_active_mon'] = df['first_active_month'].dt.month.values\n",
        "\n",
        "  # Encode first_active_year column\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  train_data['first_active_year'] = le.fit_transform(train_data['first_active_year'] )\n",
        "  test_data['first_active_year'] = le.fit_transform(test_data['first_active_year'] )\n",
        "\n",
        "  # Encode first_active_mon column\n",
        "  train_data['first_active_mon'] = le.fit_transform(train_data['first_active_mon'] )\n",
        "  test_data['first_active_mon'] = le.fit_transform(test_data['first_active_mon'] )\n",
        "\n",
        "  #2. Derive last purchase amount, last active month and dormancy feature :\n",
        "\n",
        "  ## For historical transaction\n",
        "  historical_data['purchase_date'] = pd.to_datetime(historical_data['purchase_date'])\n",
        "\n",
        "  # last active month & last purchase amount\n",
        "  last_active_month = historical_data.loc[historical_data.groupby('card_id').purchase_date.idxmax(),:][['card_id','purchase_date','purchase_amount']]\n",
        "  last_active_month.columns = ['card_id','hist_transc_last_active_purchase_date','hist_transc_last_active_purchase_amount']\n",
        "  train_data = pd.merge(train_data,last_active_month, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data,last_active_month, on=\"card_id\",how='left')\n",
        "\n",
        "  train_data['hist_transc_last_active_purchase_month'] = train_data['hist_transc_last_active_purchase_date'].dt.month\n",
        "  test_data['hist_transc_last_active_purchase_month'] = test_data['hist_transc_last_active_purchase_date'].dt.month\n",
        "\n",
        "  # dormancy feature\n",
        "  max_purchase_date_hist = historical_data['purchase_date'].max()\n",
        "  train_data['hist_transc_dormancy'] = [(max_purchase_date_hist-x).days for x in train_data['hist_transc_last_active_purchase_date']]\n",
        "  test_data['hist_transc_dormancy'] = [(max_purchase_date_hist-x).days for x in test_data['hist_transc_last_active_purchase_date']]\n",
        "  train_data.head()\n",
        "\n",
        "  ## For new_merchant_transaction\n",
        "  newmerchant_data['purchase_date'] = pd.to_datetime(newmerchant_data['purchase_date'])\n",
        "\n",
        "  # last active month & last purchase amount\n",
        "  last_active_month = newmerchant_data.loc[newmerchant_data.groupby('card_id').purchase_date.idxmax(),:][['card_id','purchase_date','purchase_amount']]\n",
        "  last_active_month.columns = ['card_id','new_transc_last_active_purchase_date','new_transc_last_active_purchase_amount']\n",
        "  train_data = pd.merge(train_data, last_active_month, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data, last_active_month, on=\"card_id\",how='left')\n",
        "\n",
        "  train_data['new_transc_last_active_purchase_month'] = train_data['new_transc_last_active_purchase_date'].dt.month\n",
        "  test_data['new_transc_last_active_purchase_month'] = test_data['new_transc_last_active_purchase_date'].dt.month\n",
        "\n",
        "  # dormancy feature\n",
        "  max_purchase_date_newmer = newmerchant_data['purchase_date'].max()\n",
        "  train_data['new_transc_dormancy'] = [(max_purchase_date_newmer-x).days for x in train_data['new_transc_last_active_purchase_date']]\n",
        "  test_data['new_transc_dormancy'] = [(max_purchase_date_newmer-x).days for x in test_data['new_transc_last_active_purchase_date']]\n",
        "\n",
        "  #3. Deriving Favourite merchant and Number of transactions at Favourite merchant as feature :\n",
        "\n",
        "  # For historical transaction\n",
        "  merchantid_counts_percard = pd.DataFrame(historical_data[['card_id','merchant_id']].groupby(['card_id','merchant_id']).size())\n",
        "  merchantid_counts_percard.columns = ['num_favourite_merchant']\n",
        "  merchantid_counts_percard = merchantid_counts_percard.sort_values(by='num_favourite_merchant',ascending=False)\n",
        "  merchantid_counts_percard = merchantid_counts_percard.groupby(level=0).head(1).reset_index()\n",
        "  merchantid_counts_percard.columns = ['card_id','hist_transc_favourite_merchant','hist_transc_num_transaction_favourite_merchant']\n",
        "  train_data = pd.merge(train_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "\n",
        "  # Encode Favourite merchant column\n",
        "  train_data['hist_transc_favourite_merchant'] = le.fit_transform(train_data['hist_transc_favourite_merchant'] )\n",
        "  test_data['hist_transc_favourite_merchant'] = le.fit_transform(test_data['hist_transc_favourite_merchant'] )\n",
        "\n",
        "  # For new_merchant_transaction\n",
        "  merchantid_counts_percard = pd.DataFrame(newmerchant_data[['card_id','merchant_id']].groupby(['card_id','merchant_id']).size())\n",
        "  merchantid_counts_percard.columns = ['num_favourite_merchant']\n",
        "  merchantid_counts_percard = merchantid_counts_percard.sort_values(by='num_favourite_merchant',ascending=False)\n",
        "  merchantid_counts_percard = merchantid_counts_percard.groupby(level=0).head(1).reset_index()\n",
        "  merchantid_counts_percard.columns = ['card_id','new_transc_favourite_merchant','new_transc_num_transaction_favourite_merchant']\n",
        "  train_data = pd.merge(train_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "  test_data = pd.merge(test_data ,merchantid_counts_percard, on=\"card_id\",how='left')\n",
        "\n",
        "  train_data['new_transc_favourite_merchant'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "  test_data['new_transc_favourite_merchant'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "\n",
        "  # Encode Favourite merchant column\n",
        "  train_data['new_transc_favourite_merchant'] = le.fit_transform(train_data['new_transc_favourite_merchant'] )\n",
        "  test_data['new_transc_favourite_merchant'] = le.fit_transform(test_data['new_transc_favourite_merchant'] )\n",
        "\n",
        "  return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40PzE9bxgM4i"
      },
      "source": [
        "**Downloading Data :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isnKLZ6NdqBH",
        "outputId": "7a266cc8-6bb5-48e6-abea-90a2d0665070"
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10445/200747/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1620215211&Signature=Mt2Std3ijid8ntLwh3anT4DJ2t%2Ba4JzlrlBq29yXAqnwJlCLBWq3qenMV6Pr6QE3ZaWIiZfBFknZQ7EcMNJJtYS2l5GrnCRu68uT%2FPaZSSWoOA4PzOBFMAwnrffU8q%2FRogni4R0j%2B%2FFBGz%2FnrHw%2B2jLG%2F4%2F7gEY%2F7jhrNptqbVJFnLPxAwGWmlU9eGutByplN3A7gfSlbGk5TybkOPPzCG%2F%2B5TSvDmkPzkyNZ3wa84dbsjCLYiSvWFT0I%2FsnazgiVXUKWJUjF6AnQ3NNBYqWrWO2Kjc0s%2F63ZIhr06MvDdbe3C%2BJgCKiL8KV7vffGjyMhq4m1bGSdaD01VyZg0x19w%3D%3D&response-content-disposition=attachment%3B+filename%3Delo-merchant-category-recommendation.zip\" -c -O 'elo-merchant-category-recommendation.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-05 10:59:00--  https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10445/200747/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1620215211&Signature=Mt2Std3ijid8ntLwh3anT4DJ2t%2Ba4JzlrlBq29yXAqnwJlCLBWq3qenMV6Pr6QE3ZaWIiZfBFknZQ7EcMNJJtYS2l5GrnCRu68uT%2FPaZSSWoOA4PzOBFMAwnrffU8q%2FRogni4R0j%2B%2FFBGz%2FnrHw%2B2jLG%2F4%2F7gEY%2F7jhrNptqbVJFnLPxAwGWmlU9eGutByplN3A7gfSlbGk5TybkOPPzCG%2F%2B5TSvDmkPzkyNZ3wa84dbsjCLYiSvWFT0I%2FsnazgiVXUKWJUjF6AnQ3NNBYqWrWO2Kjc0s%2F63ZIhr06MvDdbe3C%2BJgCKiL8KV7vffGjyMhq4m1bGSdaD01VyZg0x19w%3D%3D&response-content-disposition=attachment%3B+filename%3Delo-merchant-category-recommendation.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 644665605 (615M) [application/zip]\n",
            "Saving to: ‘elo-merchant-category-recommendation.zip’\n",
            "\n",
            "elo-merchant-catego 100%[===================>] 614.80M  70.6MB/s    in 8.5s    \n",
            "\n",
            "2021-05-05 10:59:09 (72.6 MB/s) - ‘elo-merchant-category-recommendation.zip’ saved [644665605/644665605]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP6FKoZIdrTI",
        "outputId": "f7f4b817-51e9-465e-ddf0-eb07089eca04"
      },
      "source": [
        "!unzip elo-merchant-category-recommendation.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  elo-merchant-category-recommendation.zip\n",
            "  inflating: Data Dictionary.xlsx    \n",
            "  inflating: Data_Dictionary.xlsx    \n",
            "  inflating: historical_transactions.csv  \n",
            "  inflating: merchants.csv           \n",
            "  inflating: new_merchant_transactions.csv  \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTsElnDkgVOi"
      },
      "source": [
        "**Loading data :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqIAg3ikd7ix"
      },
      "source": [
        "def loadData():\n",
        "\n",
        "\n",
        "  train_data       = pd.read_csv('/content/train.csv', parse_dates=[\"first_active_month\"])\n",
        "  test_data        = pd.read_csv('/content/test.csv', parse_dates=[\"first_active_month\"])\n",
        "  historical_data  = pd.read_csv('/content/historical_transactions.csv',parse_dates=['purchase_date'])\n",
        "  newmerchant_data = pd.read_csv('/content/new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])\n",
        "  \n",
        "  return train_data, test_data, newmerchant_data, historical_data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfekGoWJdVk4",
        "outputId": "7464b4ec-b93e-4cdb-8329-3cfcbbed7145"
      },
      "source": [
        "%%time\n",
        "train_df, test_df, newmerchant_df, historical_df = loadData()\n",
        "\n",
        "train_df = reduce_memory_usage(train_df)\n",
        "test_df = reduce_memory_usage(test_df)\n",
        "historical_df = reduce_memory_usage(historical_df)\n",
        "newmerchant_df = reduce_memory_usage(newmerchant_df)\n",
        "\n",
        "historical_df['category_3'].replace({'A':1, 'B':2,'C':3}, inplace=True)\n",
        "newmerchant_df['category_3'].replace({'A':1, 'B':2,'C':3}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to  5.20 Mb (43.7% reduction)\n",
            "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
            "Mem. usage decreased to 2026.75 Mb (34.8% reduction)\n",
            "Mem. usage decreased to 136.66 Mb (34.8% reduction)\n",
            "CPU times: user 3min 52s, sys: 11.6 s, total: 4min 4s\n",
            "Wall time: 4min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxnO7Lp-eVT4",
        "outputId": "0e72faaf-eb71-4c0d-c073-4b2c699c501e"
      },
      "source": [
        "%%time\n",
        "train_data, test_data = baseline_features(train_df,test_df,historical_df,newmerchant_df)\n",
        "\n",
        "target  = train_data[['card_id','target']]\n",
        "target.set_index('card_id', inplace =True)\n",
        "\n",
        "cards = pd.concat([train_data.drop(['target'] , axis= 1) , test_data] , axis = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 34s, sys: 1.53 s, total: 1min 36s\n",
            "Wall time: 1min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSNm1ralghdG"
      },
      "source": [
        "**Preprocessing, Designing Features, Prediction and Deployment :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqYsgIsaeqGT"
      },
      "source": [
        "def imputation(df_hist, df_new):\n",
        "  '''This function is for missing value imputation in data\n",
        "     parameters:\n",
        "     - df_test: test_data\n",
        "     - df_hist: historical_data\n",
        "     - df_new: newmerch_data.'''\n",
        "\n",
        "  # In historical_data\n",
        "  df_hist['category_2'].fillna(1,inplace=True)# I put '1' here because it is most occured value in this feature\n",
        "  df_hist['category_3'].fillna('A',inplace=True)# I put 'A' here because of most count value\n",
        "  df_hist['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)# same merchant_id is also been taken on the basis of count.\n",
        "\n",
        "  # I use same techniques for newmerchant_data\n",
        "  df_new['category_3'].fillna('A',inplace=True)\n",
        "  df_new['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "  df_new['category_2'].fillna(1.0,inplace=True)\n",
        "\n",
        "  return df_hist, df_new\n",
        "\n",
        "def encode_categorical(df):\n",
        "  '''This function is specially for encode the categorical values of \n",
        "     transactions data\n",
        "     parameters:\n",
        "     - df: the Dataframe where the label encoding will performed on certain features'''\n",
        "  \n",
        "  ## label encode the categorical variables.\n",
        "  e = {'N':0, 'Y':1}\n",
        "  df['category_1']= df['category_1'].map(e)\n",
        "  df['authorized_flag']= df['authorized_flag'].map(e)\n",
        "  return df\n",
        "\n",
        "def oneHotEncoding(df, features, original_df):\n",
        "  '''This function is for one-hot encoding the categorical features\n",
        "     parameters:\n",
        "     - df: DataFrame\n",
        "     - features: Features needs to be one hot encoded.'''\n",
        "\n",
        "  for feat in features:\n",
        "    unique_values = original_df[feat].unique()\n",
        "\n",
        "    for cat in unique_values:\n",
        "      df[feat+'={}'.format(cat)] = (df[feat] == cat).astype(int)\n",
        "\n",
        "def aggregated_features(new_df, df, aggs, grpby, name='',  prefix='', use_col=False):\n",
        "  '''\n",
        "  This function is to find the \n",
        "  aggregated values (sum,min,max,std,median,mean,nunique) for a columns aggregated by the groupby operation\n",
        "  \n",
        "  Parameters:\n",
        "  new_df   - features will be added to this DF\n",
        "  df       - original DF from which the features will be created\n",
        "  grpby    - based on this column we'll to group by\n",
        "  name     - name for the new features created\n",
        "  aggs     - dictionary contains key as the column the operation performed and list of operations as the value.\n",
        "  prefix   - added to the name of the feature -- default value empty\n",
        "  use_col  - if set True then the original column name will be uesd to name the new feature -- default value False\n",
        "  '''\n",
        "  # boolean for using the original column name in the aggregated features\n",
        "  # iterating through the columns of the need to be aggregated \n",
        "\n",
        "  for col, funcs in aggs.items():\n",
        "    for func in funcs:\n",
        "        # Getting the name of aggregation function\n",
        "        if isinstance(func, str):\n",
        "            func_str = func\n",
        "        else:\n",
        "            func_str = func.__name__ \n",
        "        # create the column\n",
        "        if use_col:\n",
        "          name = prefix+'_'+col+'_'+'{}'.format(func_str)\n",
        "\n",
        "        new_df[name] = df.groupby([grpby])[col].agg(func).values\n",
        "\n",
        "  return new_df\n",
        "\n",
        "def data_preprocessing(historical_data,newmerchant_data):\n",
        "  '''This function is for performing preprocessing \n",
        "  practices on the data.\n",
        "  parameters:\n",
        "  historical_data: data from historical_transaction\n",
        "  newmerchant_data: data from nemerch_transaction'''\n",
        "    \n",
        "  ## imputing the missing values\n",
        "  print(' - Imputing Missing values...')\n",
        "  historical_data, newmerchant_data = imputation(historical_data,newmerchant_data)\n",
        "  \n",
        "  ## encoding the categorical features in historical transactions\n",
        "  historical_data = encode_categorical(historical_data)\n",
        "\n",
        "  ## encoding the categorical features in new_merchants\n",
        "  newmerchant_data = encode_categorical(newmerchant_data)\n",
        "\n",
        "  ## One-hot encoding the categorical features\n",
        "  categorical_features = ['category_2','category_3','month_lag']\n",
        "\n",
        "  ## one-hot encoding historical transactions\n",
        "  print(' - One Hot Encoding of variables...')\n",
        "  oneHotEncoding(historical_data, features=categorical_features, original_df = historical_df)\n",
        "\n",
        "  ## one-hot encoding new merchants transactions\n",
        "  oneHotEncoding(newmerchant_data, features=categorical_features, original_df = newmerchant_df)\n",
        "  \n",
        "\n",
        "  ## calcuating month difference\n",
        "  reference_date = '2018-12-31'\n",
        "  reference_date = pd.to_datetime(reference_date)\n",
        "\n",
        "  # In historical_transactions \n",
        "  historical_data['month_diff'] = (reference_date - historical_data['purchase_date']).dt.days // (30 + historical_data['month_lag'])\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['month_diff'] = (reference_date - newmerchant_data['purchase_date']).dt.days // (30 + newmerchant_data['month_lag'])\n",
        "\n",
        "  ## Influential days feature\n",
        "  holidays = {'FathersDay_2017': '2017-08-13',\n",
        "              'ChildrenDay_2017':'2017-10-12',\n",
        "              'BlackFriday_2017':'2017-11-24',\n",
        "              'ValentineDay_2017':'2017-06-12',\n",
        "              'Republicday_2017':'2017-11-15',\n",
        "              'Independenceday_2017':'2017-09-7',\n",
        "              'EasterDay_2017' : '2017-04-16',\n",
        "              'AllSoulsDay_2017': '2017-11-2',\n",
        "              'ChristmasDay_2017': '2017-12-25'}\n",
        "\n",
        "  # In historical_transactions \n",
        "  for day, date in holidays.items():\n",
        "    historical_data[day] = (pd.to_datetime(date) - historical_data['purchase_date']).dt.days\n",
        "    historical_data[day] = historical_data[day].apply(lambda x: x if x > 0 and x < 15 else 0)\n",
        "  # In new_transactions\n",
        "  for day, date in holidays.items(): \n",
        "    newmerchant_data[day] = (pd.to_datetime(date) - newmerchant_data['purchase_date']).dt.days\n",
        "    newmerchant_data[day] = newmerchant_data[day].apply(lambda x: x if x > 0 and x < 15 else 0)\n",
        "\n",
        "  ## preprocess the purchase_amount\n",
        "  newmerchant_data['purchase_amount'] = np.round(newmerchant_data['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "  historical_data['purchase_amount'] = np.round(historical_data['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "\n",
        "  ## The ratio of purchase amount by month_diff is another feature which help to reveal the card_id's financial capcity and purchase_pattern.\n",
        "  # In historical_transactions \n",
        "  historical_data['amount_month_ratio'] = historical_data['purchase_amount'].values / (1.0 + historical_data['month_diff'].values)\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['amount_month_ratio'] = newmerchant_data['purchase_amount'].values / (1.0 + newmerchant_data['month_diff'].values)\n",
        "\n",
        "  ##is_weekend is a feature which purchase_date is weekend or weekday.\n",
        "  ##greater than 5 to check whether the day is sat or sunday then, if it is then assign a val 1 else 0\n",
        "  # In historical_transactions \n",
        "  historical_data['is_weekend'] = historical_data['purchase_date'].dt.dayofweek\n",
        "  historical_data['is_weekend'] = historical_data['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['is_weekend'] = newmerchant_data['purchase_date'].dt.dayofweek\n",
        "  newmerchant_data['is_weekend'] = newmerchant_data['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\n",
        "\n",
        "  ## extracting the day, hour , week from the purchase_date.\n",
        "  # In historical_transactions\n",
        "  historical_data['purchase_date_week'] = historical_data['purchase_date'].dt.week.values\n",
        "  historical_data['purchase_date_dayofweek'] = historical_data['purchase_date'].dt.dayofweek.values\n",
        "  historical_data['purchase_date_hour'] = historical_data['purchase_date'].dt.hour.values\n",
        "  # In new_merch_transactions\n",
        "  newmerchant_data['purchase_date_week'] = newmerchant_data['purchase_date'].dt.week.values\n",
        "  newmerchant_data['purchase_date_dayofweek'] = newmerchant_data['purchase_date'].dt.dayofweek.values\n",
        "  newmerchant_data['purchase_date_hour'] = newmerchant_data['purchase_date'].dt.hour.values\n",
        "\n",
        "  return historical_data, newmerchant_data\n",
        "\n",
        "def feature_engineering(historical_data,newmerchant_data):\n",
        "  '''This function is for performing feature engineering\n",
        "     on the data.\n",
        "     parameters:\n",
        "     - historical_data: data from historical_transaction\n",
        "     - newmerchant_data: data from nemerch_transaction'''\n",
        "  \n",
        "  # In historical_transactions \n",
        "  features_historical = pd.DataFrame(historical_data.groupby(['card_id']).size()).reset_index()\n",
        "  features_historical.columns = ['card_id', 'hist_transc_count']\n",
        "  # # In new_merch_transactions \n",
        "  features_newmerch = pd.DataFrame(newmerchant_data.groupby(['card_id']).size()).reset_index()\n",
        "  features_newmerch.columns = ['card_id', 'new_transc_count']\n",
        "  \n",
        "  ## Aggregation all the id's\n",
        "  aggs = {'city_id':['nunique'],\n",
        "          'state_id' :['nunique'],\n",
        "          'merchant_category_id':['nunique'],\n",
        "          'subsector_id':['nunique'],\n",
        "          'merchant_id':['nunique']}\n",
        "\n",
        "  # In historical_transactions        \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions \n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation Installment feature \n",
        "  aggs = {'installments':['mean', 'sum', 'max', 'min', 'std', 'skew']}\n",
        "\n",
        "  # In historical_transactions        \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation purchase_amount feature\n",
        "  aggs = {'purchase_amount':['sum', 'mean', 'max', 'min', 'median', 'std', 'skew']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id', prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id', prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation month_lag feature \n",
        "  aggs = {'month_lag': ['nunique', 'mean', 'std', 'min', 'max', 'skew']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation purchase_date feature\n",
        "  aggs = {'purchase_date': ['max','min']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation category_1 & authorized_flag features\n",
        "  aggs = {'category_1':['sum', 'mean'],\n",
        "        'authorized_flag': ['sum', 'mean']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch,newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## Aggregation category_2 & category_3 features\n",
        "  aggs = {'category_2=1.0':['sum', 'mean'],\n",
        "          'category_2=2.0':['sum', 'mean'],\n",
        "          'category_2=3.0':['sum', 'mean'],\n",
        "          'category_2=4.0':['sum', 'mean'],\n",
        "          'category_2=5.0':['sum', 'mean'],\n",
        "          'category_3=1.0':['sum', 'mean'],\n",
        "          'category_3=2.0':['sum', 'mean'],\n",
        "          'category_3=3.0':['sum', 'mean']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "  \n",
        "  ### Derived Features : from existing features\n",
        "\n",
        "  ## Authorized_flag\n",
        "  # historical_transactions\n",
        "  features_historical['hist_transc_denied_count'] = features_historical['hist_transc_count'].values - \\\n",
        "                                                    features_historical['hist_transc_authorized_flag_sum'].values\n",
        "  # new_merchant_transactions\n",
        "  features_newmerch['new_transc_denied_count'] = features_newmerch['new_transc_count'].values - \\\n",
        "                                                    features_newmerch['new_transc_authorized_flag_sum'].values\n",
        "\n",
        "  ## category_1\n",
        "  # historical_transactions\n",
        "  features_historical['hist_transc_category_1_sum_0'] = features_historical['hist_transc_count'].values - \\\n",
        "                                                              features_historical['hist_transc_category_1_sum'].values\n",
        "  # new_merchant_transactions\n",
        "  features_newmerch['new_transc_category_1_sum_0'] = features_newmerch['new_transc_count'].values - \\\n",
        "                                                            features_newmerch['new_transc_category_1_sum'].values\n",
        "  \n",
        "  ## find mean of the count of the transac for merchant id\n",
        "  # historical_transactions\n",
        "  features_historical['hist_transc_merchant_id_count_mean'] = features_historical['hist_transc_count'].values \\\n",
        "                                                                    / (1.0+features_historical['hist_transc_merchant_id_nunique'].values)\n",
        "  # new_merchant_transactions\n",
        "  features_newmerch['new_transc_merchant_id_count_mean'] = features_newmerch['new_transc_count'].values \\\n",
        "                                                              / (1.0+features_newmerch['new_transc_merchant_id_nunique'].values)\n",
        "  \n",
        "  ## In historical_transactions\n",
        "  # diff in purchase_date from max to min \n",
        "  features_historical['hist_transc_purchase_date_diff'] = (features_historical['hist_transc_purchase_date_max'] - features_historical['hist_transc_purchase_date_min']).dt.days.values\n",
        "  # purchase_count_ratio\n",
        "  features_historical['hist_transc_purchase_count_ratio'] = features_historical['hist_transc_count'].values / (1.0 + features_historical['hist_transc_purchase_date_diff'].values)\n",
        "\n",
        "  ## In new_merch_transactions\n",
        "  # diff in purchase_date from max to min \n",
        "  features_newmerch['new_transc_purchase_date_diff'] = (features_newmerch['new_transc_purchase_date_max'] - features_newmerch['new_transc_purchase_date_min']).dt.days.values\n",
        "  # purchase_count_ratio\n",
        "  features_newmerch['new_transc_purchase_count_ratio'] = features_newmerch['new_transc_count'].values / (1.0 + features_newmerch['new_transc_purchase_date_diff'].values)\n",
        "\n",
        "  ## aggregate features for is_weekend \n",
        "  aggs = {'is_weekend': ['sum','mean']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='purchase_is_weekend_', use_col=True)\n",
        "\n",
        "  ## aggregated features on  day, hour , week\n",
        "  aggs = {'purchase_date_week': ['nunique', 'mean', 'min', 'max'],\n",
        "          'purchase_date_dayofweek': ['nunique', 'mean', 'min', 'max'],\n",
        "          'purchase_date_hour':['nunique', 'mean', 'min', 'max']}\n",
        "\n",
        "  # In historical_transactions\n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  ## calculating the aggerated features like ['mean', 'min', 'max'] for month_diff column.\n",
        "  aggs = {'month_diff': ['mean', 'min', 'max']}\n",
        "\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id', prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id', prefix='new_transc', use_col=True)\n",
        "\n",
        "  # aggregated features on the amount ratio and month_lag.\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\n",
        "          'month_lag=0': ['sum','mean'],\n",
        "          'month_lag=-1':['sum','mean'],\n",
        "          'month_lag=-2':['sum','mean']}\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "\n",
        "  # aggregated features on the amount ratio and month_lag.\n",
        "  aggs = {'amount_month_ratio': ['mean', 'std', 'min', 'max', 'skew'],\n",
        "          'month_lag=1': ['sum','mean'],\n",
        "          'month_lag=2':['sum','mean']}\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "\n",
        "  # month_lag ratio in historical transactions\n",
        "  features_historical['hist_transc_month_lag_0_-1_ratio'] = features_historical['hist_transc_month_lag=0_sum'] \\\n",
        "                                                                  / (1.0 + features_historical['hist_transc_month_lag=-1_sum'])\n",
        "\n",
        "  features_historical['hist_transc_month_lag_0_-2_ratio'] = features_historical['hist_transc_month_lag=0_sum'] \\\n",
        "                                                                  / (1.0 + features_historical['hist_transc_month_lag=-2_sum'])\n",
        "\n",
        "  # ratio of the summed month lags with the transaction count\n",
        "  tmp = features_historical[['hist_transc_month_lag=0_sum','hist_transc_month_lag=-1_sum','hist_transc_month_lag=-2_sum']].sum(axis=1)\n",
        "\n",
        "  features_historical['hist_transc_month_lag_sum_ratio'] = tmp / (1.0 + features_historical['hist_transc_count'])\n",
        "\n",
        "  # month_lag ratio between two month_lags.\n",
        "  features_newmerch['new_transc_month_lag_1_2_ratio'] = features_newmerch['new_transc_month_lag=1_sum'] \\\n",
        "                                                                  / (1.0 + features_newmerch['new_transc_month_lag=2_sum'])\n",
        "\n",
        "  ## difference in the amount spend with cards\n",
        "  # In historical_transactions \n",
        "  features_historical['hist_transc_amount_diff'] = features_historical['hist_transc_purchase_amount_max'].values - features_historical['hist_transc_purchase_amount_min'].values\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch['new_transc_amount_diff'] = features_newmerch['new_transc_purchase_amount_max'].values - features_newmerch['new_transc_purchase_amount_min'].values\n",
        "\n",
        "  ## Influential days feature\n",
        "  holidays = {'FathersDay_2017': '2017-08-13',\n",
        "              'ChildrenDay_2017':'2017-10-12',\n",
        "              'BlackFriday_2017':'2017-11-24',\n",
        "              'ValentineDay_2017':'2017-06-12',\n",
        "              'Republicday_2017':'2017-11-15',\n",
        "              'Independenceday_2017':'2017-09-7',\n",
        "              'EasterDay_2017' : '2017-04-16',\n",
        "              'AllSoulsDay_2017': '2017-11-2',\n",
        "              'ChristmasDay_2017': '2017-12-25'}\n",
        "\n",
        "  ## aggregation of holidays\n",
        "  aggs = dict(zip(holidays.keys(),[['mean'] for x in holidays.keys()]))\n",
        "  # In historical_transactions \n",
        "  features_historical = aggregated_features(features_historical, historical_data, aggs, grpby='card_id',prefix='hist_transc', use_col=True)\n",
        "  # In new_merch_transactions\n",
        "  features_newmerch = aggregated_features(features_newmerch, newmerchant_data, aggs, grpby='card_id',prefix='new_transc', use_col=True)\n",
        "  \n",
        "  return features_historical, features_newmerch\n",
        "\n",
        "def merge_drop(df,features_historical,features_newmerch):\n",
        "   '''This function is for merging the engineered features and\n",
        "      removing the unnecessary features.\n",
        "      parameters:\n",
        "      - df: data for prediction\n",
        "      - features_historical: features from historical_transc\n",
        "      - features_newmerch: features from nemerch_transc'''\n",
        "\n",
        "  train = reduce(lambda left,right: pd.merge(left,right,on='card_id', how='left'), [df, features_historical, features_newmerch])\n",
        "  \n",
        "  remove_cols = ['first_active_month','new_transc_purchase_date_max',\n",
        " 'new_transc_purchase_date_min','hist_transc_purchase_date_max',\n",
        " 'hist_transc_purchase_date_min', 'hist_transc_last_active_purchase_date', 'new_transc_last_active_purchase_date']\n",
        "\n",
        "  train = train.drop(labels=remove_cols, axis = 1)\n",
        "\n",
        "  return train\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def loyalty_score():\n",
        "  return flask.render_template('index.html')\n",
        "\n",
        "@app.route('/predict',methods = ['POST'] )\n",
        "def predict_loyalty_score():\n",
        "  ''' This function predict's loyalty Score of the given card_id/ids\n",
        "      parameters:\n",
        "      - X : card_id/List of the card_ids\n",
        "      returns Dataframe with predicted loyalty score for card_id/ids.'''\n",
        "  \n",
        "  card_id = request.form.to_dict()\n",
        "  X = card_id['card_id']\n",
        "\n",
        "  X = [str(X)]\n",
        "\n",
        "  ## training the data \n",
        "  print(\"Fetching the transactional and card_id data\")\n",
        "  sample = cards.loc[cards['card_id'].isin(X)]\n",
        "  newmerchant_data = newmerchant_df[newmerchant_df['card_id'].isin(X)]\n",
        "  historical_data = historical_df[historical_df['card_id'].isin(X)]\n",
        "  \n",
        "  ## Performing preprocessing steps on transactions data\n",
        "  print(\"PreProcess the transactions data......\")\n",
        "  historical_data, newmerchant_data = data_preprocessing(historical_data,newmerchant_data)\n",
        "\n",
        "  ## Performing feature Engineering on transactions data\n",
        "  print(\"Feature Engineering the transactions data.....\")\n",
        "  features_historical, features_newmerch = feature_engineering(historical_data,newmerchant_data)\n",
        "\n",
        "  ## Performing preprocessing on Feature Engineered Data\n",
        "  print('preprocess the Feature Engineered Data')\n",
        "  query_point = merge_drop(sample,features_historical,features_newmerch)\n",
        "  query_point.set_index('card_id', inplace=True)\n",
        "\n",
        "  ## Predicting the Loyalty Score\n",
        "  print(\"Predicting the Loyalty Score.....\")\n",
        "  with open('/content/drive/MyDrive/Case study1/xgb_model_final.sav', 'rb') as pickle_file:\n",
        "      xgboost_model = pickle.load(pickle_file)\n",
        "  with open('/content/drive/MyDrive/Case study1/lgbm_model1.sav', 'rb') as pickle_file:\n",
        "    lgbm_model = pickle.load(pickle_file)\n",
        "  with open('/content/drive/MyDrive/Case study1/stacked_model1.sav', 'rb') as pickle_file:\n",
        "      stacked_model = pickle.load(pickle_file)\n",
        "\n",
        "  xgboost_predictions = xgboost_model.predict(xgb.DMatrix(query_point[xgboost_model.feature_names]), ntree_limit=xgboost_model.best_ntree_limit+50)\n",
        "  lgbm_predictions = lgbm_model.predict(query_point, num_iteration=lgbm_model.best_iteration)\n",
        "  stacked_prediction = np.vstack([xgboost_predictions, lgbm_predictions]).transpose()\n",
        "  final_prediction = stacked_model.predict(stacked_prediction)\n",
        "  \n",
        "  score = round(final_prediction[0],6)\n",
        "  \n",
        "  return flask.render_template('index.html',  prediction_text = 'Predicted loyality score: {}'.format(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUhrIenTiItR",
        "outputId": "8ff51bf2-0ee3-41cd-ce0d-904b8ca59fd4"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://1f347ad544b7.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:31:47] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [05/May/2021 11:31:47] \"\u001b[37mGET /static/css/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [05/May/2021 11:31:47] \"\u001b[37mGET /static/elo.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [05/May/2021 11:31:47] \"\u001b[37mGET /static/background.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [05/May/2021 11:31:49] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:31:56] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:31:58] \"\u001b[31m\u001b[1mGET /predict HTTP/1.1\u001b[0m\" 405 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:32:15] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:34:01] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n",
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n",
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:34:28] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching the transactional and card_id data\n",
            "PreProcess the transactions data......\n",
            " - Imputing Missing values...\n",
            " - One Hot Encoding of variables...\n",
            "Feature Engineering the transactions data.....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [05/May/2021 11:34:42] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess the Feature Engineered Data\n",
            "Predicting the Loyalty Score.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXY--tOJnB32"
      },
      "source": [
        "**You can check the deployment video here : https://youtu.be/D0A13SzI0_8**"
      ]
    }
  ]
}